KNN Iterations to choose best K from 1<=K<=20

K: 1, accuracy: 0.7384384634613782
K: 2, accuracy: 0.7435213732188984
K: 3, accuracy: 0.7574368802599784
K: 4, accuracy: 0.7678526789434214
K: 5, accuracy: 0.7681859845012916
K: 6, accuracy: 0.7745187901008249
K: 7, accuracy: 0.7729355887009416
K: 8, accuracy: 0.7774352137321889
K: 9, accuracy: 0.7742688109324223
K: 10, accuracy: 0.7810182484792934
K: 11, accuracy: 0.7776851929005916
K: 12, accuracy: 0.7854345471210732
K: 13, accuracy: 0.783101408215982
K: 14, accuracy: 0.7866844429630864
K: 15, accuracy: 0.784934588784268
K: 16, accuracy: 0.78860094992084
K: 17, accuracy: 0.7873510540788268
K: 18, accuracy: 0.7893508874260479
K: 19, accuracy: 0.7856011999000083
K: 20, accuracy: 0.7916006999416715


=============LOGISTIC REGRESSION/DEFAULT PARAMETERS============
Precision: 87.297
Recall: 91.197
Accuracy: 88.968
F1 Score: 89.204

=============RANDOM FOREST/DEFAULT PARAMETERS============
Precision: 74.310
Recall: 77.643
Accuracy: 75.410
F1 Score: 75.940
============================================

LogisticRegression(C=0.1, penalty='l1', solver='liblinear')
['Av. Precision: 82.34758234057462', 'Av. Recall: 87.08314806580701', 'Av. Accuracy: 84.21481481481483', 'Av. F1 Score: 84.64918599625413']

LogisticRegression(C=0.1, penalty='l2', solver='liblinear')
['Av. Precision: 87.0020063055317', 'Av. Recall: 89.98073217726396', 'Av. Accuracy: 88.27407407407406', 'Av. F1 Score: 88.46630236794171']

LogisticRegression(C=0.1, penalty='l1', solver='saga')
['Av. Precision: 82.46867358546916', 'Av. Recall: 88.3748332592263', 'Av. Accuracy: 84.80074074074074', 'Av. F1 Score: 85.31966333883405']

LogisticRegression(C=0.1, penalty='l2', solver='saga')
['Av. Precision: 86.8016310179555', 'Av. Recall: 89.92144656884543', 'Av. Accuracy: 88.1296296296296', 'Av. F1 Score: 88.33400065518872']


LogisticRegression(C=0.1, penalty='l2', solver='sag')
['Av. Precision: 86.864604149636', 'Av. Recall: 89.82510745516525', 'Av. Accuracy: 88.12629629629629', 'Av. F1 Score: 88.3200536128786']

LOGISTIC REGRESSION PARAM SEARCH

@param-grid
C = [0.001, 0.01, 0.1, 1, 10, 100] #inverse of regularization strength -> reduce overfiting by reducing the variance
solver = ['lbfgs', 'liblinear', 'newton-cg', 'saga'] #optimization algorithm 
penalty = ['none','l1', 'l2'] #type of regularization. 

@results
Best Penalty: l1
Best Solver: liblinear
Best C: 1

LogisticRegression(C=1, penalty='l1', solver='liblinear')

METRICS
Precision: 88.606
Recall: 90.826
Accuracy: 89.578
F1 Score: 89.702
===============================================================================================================
KNEAREST NEIGHBORS PARAM SEARCH

@param-grid
'n_neighbors': [5, 7, 10, 15],
'weights': ['uniform', 'distance']

@results
Best N. Neighbors: 10
Best Weights: distance

KNeighborsClassifier(n_neighbors=10, weights='uniform')

METRICS
Precision: 79.533
Recall: 83.482
Accuracy: 81.007
F1 Score: 81.459

===============================================================================================================
RANDOM FOREST PARAM SEARCH

@param-grid
'n_estimators': [25, 50, 75, 100, 125, 150, 175, 200]
'max_depth': [10, 20, 30]
'min_samples_split': [2, 5, 10]
'min_samples_leaf': [1, 2, 4]


@results
Best N. Max Depth: 30
Best Min Samples Leaf: 1
Best Min Samples Split: 2
Best N Estimators: 200

RandomForestClassifier(max_depth=30, random_state=42,min_samples_leaf=1, min_samples_split=2, n_estimators=200)

METRICS
Precision: 90.009
Recall: 93.538
Accuracy: 91.581
F1 Score: 91.740

SECOND VALIDATION
'n_estimators': [300, 400, 450, 500, 550, 600],
'max_depth': [50, 60, 70, 80, 90, 100],

@results
Best N. Max Depth: 400
Best N Estimators: 60

METRICS:
Precision: 93.137
Recall: 94.938
Accuracy: 93.974
F1 Score: 94.029

===============================================================================================================
SUPPORT VECTOR MACHINE PARAM SEARCH

@param-grid
'C': [0.1, 1, 10],
'gamma': [0.1, 1, 10],
'kernel': ['linear', 'rbf', 'sigmoid']

@results
Best C: 1
Best Gamma: 0.1
Best Kernel: rbf

SVC(C = 1, kernel = 'rbf', gamma = 0.1, random_state=42)

METRICS
Precision: 89.794
Recall: 92.515
Accuracy: 91.004
F1 Score: 91.134



Metricas finales tras busqueda de hiperpar√°metros. 

=============LOGISTIC REGRESSION============
Precision: 89.246
Recall: 91.144
Accuracy: 90.085
F1 Score: 90.185
============================================
=============K-NEAREST NEIGHBORS============
Precision: 78.646
Recall: 81.118
Accuracy: 79.556
F1 Score: 79.863
============================================
=============RANDOM FOREST============
Precision: 93.341
Recall: 95.353
Accuracy: 94.278
F1 Score: 94.336
============================================
=============SUPPORT VECTOR MACHINE============
Precision: 89.154
Recall: 91.618
Accuracy: 90.241
F1 Score: 90.370
============================================